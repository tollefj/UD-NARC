{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udtools_path = \"C:/Users/tollef/Documents/Git/UDTOOLS/\"\n",
    "udtools_path = \"/Users/tollef/Downloads/git/PHD/COREF/UniversalDepTools/\"\n",
    "validate_path = os.path.join(udtools_path, \"validate.py\")\n",
    "# ud_narc_path = \"NARC/UD_NARC_MERGED_bokmaal\"\n",
    "ud_narc_path = \"NARC/UD_NARC_MERGED_nynorsk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On split test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:03<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On split train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 342/342 [00:42<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On split dev...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:03<00:00,  7.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for split_folder in os.listdir(ud_narc_path):\n",
    "    doc_results = {}\n",
    "\n",
    "    split_path = os.path.join(ud_narc_path, split_folder)\n",
    "    print(f\"On split {split_folder}...\")\n",
    "    for file in tqdm(os.listdir(split_path)):\n",
    "        if \".conllu\" not in file:\n",
    "            continue\n",
    "        narc_file = os.path.join(split_path, file)\n",
    "        doc = file.split(\".\")[0]\n",
    "\n",
    "        # run the validate script and get the outputs!\n",
    "        output = subprocess.run([\"python\", validate_path, \"--lang\", \"no\", \"--coref\", \"--level\", \"2\", narc_file], capture_output=True)\n",
    "        # get stderr and stdout\n",
    "        output = output.stderr.decode(\"utf-8\")  # everything is in stderr\n",
    "        stdout = output.split(\"\\n\")\n",
    "        doc_results[doc] = stdout\n",
    "\n",
    "    all_results[split_folder] = doc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for test split:\n",
      "4 documents with coref errors\n",
      "0 documents with warnings\n",
      "27 documents passed\n",
      "Should remove the following documents from dev split:\n",
      "dict_keys(['vtbnn~20090625-4277', 'kknn~20050406-2630', 'firdann~20110118-5455276', 'kknn~20060803-38315'])\n",
      "________________________________________\n",
      "Stats for train split:\n",
      "20 documents with coref errors\n",
      "40 documents with warnings\n",
      "322 documents passed\n",
      "Should remove the following documents from dev split:\n",
      "dict_keys(['vtbnn~20030930-1531', 'vtbnn~20020413-809', 'kknn~20050628-5798', 'kknn~20100628-57640', 'firdann~20110902-5720916', 'vtbnn~20040401-1780', 'dot~20110930-2119', 'dot~20060901-1348', 'dot~20111021-2134', 'kknn~20050201-844', 'dot~20110923-2116', 'mom~mom_002', 'dot~20051126-525', 'dot~20110916-2111', 'vtbnn~20030902-1488', 'firdann~20100415-5072393', 'kknn~20110825-59206', 'dot~20111021-2135', 'dot~20060901-845', 'firdann~20100305-5006815'])\n",
      "________________________________________\n",
      "Stats for dev split:\n",
      "2 documents with coref errors\n",
      "2 documents with warnings\n",
      "23 documents passed\n",
      "Should remove the following documents from dev split:\n",
      "dict_keys(['vtbnn~20070403-3234', 'vtbnn~20090623-4272'])\n",
      "________________________________________\n"
     ]
    }
   ],
   "source": [
    "err_docs = []\n",
    "errors = []\n",
    "for split, doc_results in all_results.items():\n",
    "    coref_errs = defaultdict(list)\n",
    "    warnings = defaultdict(list)\n",
    "    passed = []\n",
    "\n",
    "    print(f\"Stats for {split} split:\")\n",
    "    for doc, results in doc_results.items():\n",
    "        for line in results:\n",
    "            if \"PASSED\" in line:\n",
    "                passed.append(doc)\n",
    "            elif \"Coref\" in line:\n",
    "                coref_errs[doc].append(line)\n",
    "            elif \"Warning\" in line:\n",
    "                warnings[doc].append(line)\n",
    "    print(f\"{len(coref_errs)} documents with coref errors\")\n",
    "    print(f\"{len(warnings)} documents with warnings\")\n",
    "    print(f\"{len(passed)} documents passed\")\n",
    "\n",
    "    print(f\"Should remove the following documents from {split_folder} split:\")\n",
    "    print(coref_errs.keys())\n",
    "    errors.append(coref_errs)\n",
    "    err_docs.extend(coref_errs.keys())\n",
    "    print(\"_\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kknn~20060803_38315__T365',\n",
       " 'kknn~20050201_844__T91',\n",
       " 'dot~20110923_2116__T18',\n",
       " 'kknn~20100628_57640__T269',\n",
       " 'kknn~20050628_5798__T35',\n",
       " 'vtbnn~20090625_4277__T7',\n",
       " 'vtbnn~20040401_1780__3256',\n",
       " 'firdann~20110118_5455276__T9',\n",
       " 'firdann~20100305_5006815__269',\n",
       " 'vtbnn~20070403_3234__T254',\n",
       " 'firdann~20110902_5720916__T111',\n",
       " 'dot~20110930_2119__T111',\n",
       " 'vtbnn~20030930_1531__T315',\n",
       " 'vtbnn~20020413_809__T146',\n",
       " 'dot~20060901_1348__T278',\n",
       " 'dot~20110916_2111__T11',\n",
       " 'vtbnn~20030902_1488__91488',\n",
       " 'vtbnn~20030902_1488__3719',\n",
       " 'firdann~20100305_5006815__13802',\n",
       " 'dot~20111021_2135__217',\n",
       " 'kknn~20050406_2630__T22',\n",
       " 'mom~mom_002__T58',\n",
       " 'firdann~20100415_5072393__T41',\n",
       " 'dot~20060901_845__T171',\n",
       " 'vtbnn~20090625_4277__T8',\n",
       " 'vtbnn~20090625_4277__T10',\n",
       " 'vtbnn~20030902_1488__91488',\n",
       " 'vtbnn~20030902_1488__3719',\n",
       " 'vtbnn~20030902_1488__91488',\n",
       " 'vtbnn~20030902_1488__3719',\n",
       " 'dot~20051126_525__14941',\n",
       " 'kknn~20110825_59206__T182',\n",
       " 'kknn~20110825_59206__T52',\n",
       " 'vtbnn~20090623_4272__T25',\n",
       " 'dot~20111021_2134__T173']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "entity_pattern = re.compile(r\"__T\\d+|__\\d+\")\n",
    "between_apos = re.compile(r\"'(.*?)'\")\n",
    "def entity_filter(ent):\n",
    "    ent = ent.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    ent = ent.replace(\"--1-\", \"\")\n",
    "    # remove anything between brackets: []\n",
    "    ent = re.sub(r\"\\[.*?\\]\", \"\", ent)\n",
    "    return ent\n",
    "\n",
    "filtered_errors = []\n",
    "for error in errors:\n",
    "    for doc, coreferr in error.items():\n",
    "        for _err in coreferr:\n",
    "            candidates = between_apos.findall(_err)\n",
    "            candidates = [entity_filter(c) for c in candidates if \"__\" in c]\n",
    "            if len(candidates) > 0:\n",
    "                filtered_errors.extend(candidates)\n",
    "\n",
    "filtered_errors = list(set(filtered_errors))\n",
    "with_splits = []\n",
    "for fe in filtered_errors:\n",
    "    if \"<\" in fe:\n",
    "        all_splits = fe.split(\",\")\n",
    "        for _split in all_splits:\n",
    "            start, end = _split.split(\"<\")\n",
    "            with_splits.append(start)\n",
    "            with_splits.append(end)\n",
    "    else:\n",
    "        with_splits.append(fe)\n",
    "with_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write these to a file: nynorsk_invalid_entities.txt\n",
    "with open(\"invalid_entities.txt\", \"a\") as f:\n",
    "    # newline separated list above:\n",
    "    f.writelines(\"\\n\".join(sorted(with_splits)))\n",
    "    f.writelines(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vtbnn~20030902_1488__91488', 'vtbnn~20030902_1488__3719']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'vtbnn~20030902_1488__91488<vtbnn~20030902_1488__3719'.split(\"<\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results before change 02.02.23:\n",
    " 2 (test), 10 (train), 1 (dev)\n",
    " \n",
    " 0, 3, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dot~20051126-525',\n",
       " 'dot~20110916-2111',\n",
       " 'dot~20111021-2135',\n",
       " 'dot~20111104-2146',\n",
       " 'vtbnn~20030902-1488',\n",
       " 'vtbnn~20040401-1780']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(err_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if bokmaal:\n",
    "# with open(\"invalid_docs.txt\", \"w\") as f:\n",
    "#     f.writelines(\"\\n\".join(sorted(err_docs)))\n",
    "\n",
    "with open(\"invalid_docs.txt\", \"a\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    f.writelines(\"\\n\".join(sorted(err_docs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29f1e52c0d8d5e5ede6aaca4be8238d35b46afd62a3b8286547e2768de775769"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
