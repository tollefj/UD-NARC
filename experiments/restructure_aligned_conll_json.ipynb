{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we want to take the aligned corpus (NarNE) and convert it\n",
    "# to a jsonlines format conforming to word-level coreference:\n",
    "# https://github.com/vdobrovolskii/wl-coref/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "import os\n",
    "\n",
    "path = \"../output/aligned/no-narc_bokmaal/narc_bokmaal_dev.conllu\"\n",
    "\n",
    "data = conllu.parse(open(path).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenList<Pinefull, død, for, den, irske, pub, ?, metadata={newdoc id: \"ap~20050210-762251\", global.Entity: \"eid-etype-head-other\", sent_id: \"016056\", text: \"Pinefull død for den irske pub?\"}>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data[0]\n",
    "sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -> Jsonlines\n",
    "\n",
    "For each document, we want...\n",
    "```\n",
    "    document_id:    str,\n",
    "    cased_words:    [str, ...]                # words\n",
    "    sent_id:        [int, ...]                # word id to sent id\n",
    "    part_id:        [int, ...]                # word id to part id\n",
    "    speaker:        [str, ...]                # word id to speaker\n",
    "    pos:            [str, ...]                # word id to POS\n",
    "    deprel:         [str, ...]                # word id to dep. relation\n",
    "    head:           [int, ...]                # word id to head, None for root\n",
    "    clusters:       [[[int, int], ...], ...]  # list of clusters, where each\n",
    "                                                cluster is\n",
    "                                                a list of spans of words\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"newdoc id\" in data[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new document will always start with \"newdoc id\" in the metadata field\n",
    "def is_new_doc(sample):\n",
    "    return \"newdoc id\" in sample.metadata\n",
    "# group all documents by their id.\n",
    "from collections import defaultdict\n",
    "grouped_docs = defaultdict(list)\n",
    "current_doc = None\n",
    "for sent in data:\n",
    "    if is_new_doc(sent):\n",
    "        current_doc = sent.metadata[\"newdoc id\"]\n",
    "    grouped_docs[current_doc].append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ap~20050210-762251', 'ap~20081210-1411542', 'ap~20081210-1546270', 'ap~20081210-1564010', 'ap~20081210-1775472', 'ap~20090401-3010501', 'ap~20090803-3199497', 'ap~20090805-3202217', 'ap~20090825-3233467', 'ap~20090905-3252356', 'ap~20090911-3262518', 'ap~20091022-3333021', 'bt~BT-20120916-2765289a', 'db~20081117-3745306', 'db~20081118-3754590', 'db~20081118-3758669', 'db~20081118-3759012', 'db~20081128-3858534a', 'db~20081202-3901555', 'db~20081206-3954583', 'kk~20110826-59215', 'kk~20110827-59218', 'spbm~20050822-508220301', 'spbm~20050822-508220303', 'spbm~20050822-508220304', 'spbm~20050822-508220308', 'spbm~20050822-508220309', 'spbm~20050822-508220311', 'spbm~20050822-508220313', 'vg~VG-20121202-10056280', 'vg~VG-20121211-10071599'])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_docs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'form': 'Pinefull',\n",
       " 'lemma': 'pinefull',\n",
       " 'upos': 'ADJ',\n",
       " 'xpos': None,\n",
       " 'feats': {'Definite': 'Ind', 'Degree': 'Pos', 'Number': 'Sing'},\n",
       " 'head': 2,\n",
       " 'deprel': 'amod',\n",
       " 'deps': None,\n",
       " 'misc': {'name': 'O', 'Entity': '(1'}}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_docs['ap~20050210-762251'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "string = \"(vg~vg_20111003_10039641__2401--1-(vg~vg_20111003_10039641__81112--1-)\"\n",
    "regex = r\"[a-zA-Z]+__(\\d+)\\-\\(\\S+?__(\\d+)\"\n",
    "matches = re.findall(regex, string)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define a regular expression pattern to match CoNLL-style mention annotations\n",
    "CONLL_MENTION_PATTERN = re.compile(r'(?:\\((?P<mono>\\d+)\\)|\\((?P<start>\\d+)|(?P<end>\\d+)\\))')\n",
    "\n",
    "def compute_mentions(columns):\n",
    "    # Use a dictionary to keep track of pending mentions waiting for an \"end\" tag\n",
    "    pending = defaultdict(list)\n",
    "    mentions = []\n",
    "    \n",
    "    # Iterate over each column in the input\n",
    "    for i, col in enumerate(columns):\n",
    "        # Find all CoNLL-style mention annotations in the column using the regex pattern\n",
    "        for m in CONLL_MENTION_PATTERN.finditer(col):\n",
    "            # If the annotation is a singleton mention, add it to the mentions list\n",
    "            if m.lastgroup == 'mono':\n",
    "                pos = (i, i+1)\n",
    "                chain = int(m.group(m.lastgroup))\n",
    "                mentions.append((pos, chain))\n",
    "            # If the annotation is a start tag, add it to the pending list\n",
    "            elif m.lastgroup == 'start':\n",
    "                chain = int(m.group(m.lastgroup))\n",
    "                if chain not in pending:\n",
    "                    pending[chain] = []\n",
    "                pending[chain].append(i)\n",
    "            # If the annotation is an end tag, retrieve the corresponding start tag\n",
    "            # from the pending list and add the mention to the mentions list\n",
    "            elif m.lastgroup == 'end':\n",
    "                chain = int(m.group(m.lastgroup))\n",
    "                pos = (pending[chain].pop(), i+1)\n",
    "                mentions.append((pos, chain))\n",
    "            else:\n",
    "                # This should never happen\n",
    "                assert False\n",
    "\n",
    "    return mentions\n",
    "\n",
    "def compute_chains(columns):\n",
    "    # Use a dictionary to keep track of chains, where the key is the chain ID\n",
    "    # and the value is a list of (start, end) tuples representing the spans of\n",
    "    # the mentions in the chain\n",
    "    chains = dict()\n",
    "    \n",
    "    # Iterate over each mention in the input and add it to the appropriate chain\n",
    "    for (start, stop), chain_id in compute_mentions(columns):\n",
    "        end = stop - 1\n",
    "        if chain_id not in chains:\n",
    "            chains[chain_id] = []\n",
    "        chains[chain_id].append((start, end))\n",
    "    \n",
    "    # Return the list of chains (i.e., the values of the chains dictionary)\n",
    "    return list(chains.values())\n",
    "\n",
    "\n",
    "def get_coref_clusters_from_doc(doc):\n",
    "    misc = []\n",
    "    for s_id, sent in enumerate(doc):\n",
    "        sent_misc = []\n",
    "        for token in sent:\n",
    "            _misc = token.get(\"misc\", None)\n",
    "            entity = _misc.get(\"Entity\", None)\n",
    "            sent_misc.append(entity if entity else \"*\")\n",
    "        misc.extend(sent_misc)\n",
    "    \n",
    "    clusters = compute_chains(misc)\n",
    "    return clusters\n",
    "\n",
    "ents = get_coref_clusters_from_doc(grouped_docs['ap~20050210-762251'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert all the entity data in the misc column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def get_head(mention: Tuple[int, int], heads: List[int]) -> int:\n",
    "    \"\"\"Returns the span's head, which is defined as the only word within the\n",
    "    span whose head is outside of the span or None. In case there are no or\n",
    "    several such words, the rightmost word is returned\n",
    "    Args:\n",
    "        mention (Tuple[int, int]): start and end (exclusive) of a span\n",
    "        doc (dict): the document data\n",
    "    Returns:\n",
    "        int: word id of the spans' head\n",
    "    \"\"\"\n",
    "    head_candidates = set()\n",
    "    start, end = mention\n",
    "    for i in range(start, end):\n",
    "        ith_head = heads[i]\n",
    "        if ith_head is None or not (start <= ith_head < end):\n",
    "            head_candidates.add(i)\n",
    "    if len(head_candidates) == 1:\n",
    "        return head_candidates.pop()\n",
    "    return end - 1\n",
    "\n",
    "\n",
    "def parse_doc(doc, part_id=None):\n",
    "    \"\"\"_summary_\n",
    "    Args:\n",
    "        doc (_type_): documents corresponding to a single document id\n",
    "        part_id (_type_, optional): just the document counter\n",
    "    Returns:\n",
    "        _type_: jsonlines formatted data\n",
    "    \"\"\"\n",
    "    doc_id = doc[0].metadata[\"newdoc id\"]\n",
    "    cased_words = [word[\"form\"] for sent in doc for word in sent]\n",
    "\n",
    "    sent_id = []  # create a sentence mapping starting from 0\n",
    "    current_sent = 0\n",
    "    for sent in doc:\n",
    "        sent_id.extend([current_sent] * len(sent))\n",
    "        current_sent += 1\n",
    "\n",
    "    speaker = [0 * len(sent) for sent in doc]\n",
    "\n",
    "    pos = [word[\"upos\"] for sent in doc for word in sent]\n",
    "    deprel = [word[\"deprel\"] for sent in doc for word in sent]\n",
    "    heads = [word[\"head\"] for sent in doc for word in sent]\n",
    "    heads = [None if n == b else b for n, b in enumerate(heads)]\n",
    "\n",
    "    # now we need to group all coreference clusters...\n",
    "    clusters = get_coref_clusters_from_doc(doc)\n",
    "\n",
    "    ###############\n",
    "    # this is the \"to_heads\" functionality from wl-coref\n",
    "    ###############\n",
    "    head_clusters = [\n",
    "        [get_head(mention, heads) for mention in cluster]\n",
    "        for cluster in clusters\n",
    "    ]\n",
    "    # check for duplicates\n",
    "    head2spans = defaultdict(list)\n",
    "    for cluster, head_cluster in zip(clusters, head_clusters):\n",
    "        for span, span_head in zip(cluster, head_cluster):\n",
    "            head2spans[span_head].append((span, head_cluster))\n",
    "\n",
    "    filtered_head2spans = []\n",
    "    for head, spans in head2spans.items():\n",
    "        spans.sort(key=lambda x: x[0][1] - x[0][0])  # shortest spans first\n",
    "        filtered_head2spans.append((head, *spans[0][0]))\n",
    "        if len(spans) > 1:\n",
    "            for _, cluster in spans[1:]:\n",
    "                cluster.remove(head)\n",
    "\n",
    "    filtered_head_clusters = [\n",
    "        cluster for cluster in head_clusters if len(cluster) > 1]\n",
    "\n",
    "    data = {\n",
    "        \"document_id\":      doc_id,\n",
    "        \"cased_words\":      cased_words,\n",
    "        \"sent_id\":          sent_id,\n",
    "        \"part_id\":          part_id,\n",
    "        \"speaker\":          speaker,\n",
    "        \"pos\":              pos,\n",
    "        \"deprel\":           deprel,\n",
    "        \"head\":             heads,\n",
    "        \"span_clusters\":    clusters,\n",
    "        \"word_clusters\":    filtered_head_clusters,\n",
    "        \"head2span\":        filtered_head2spans,\n",
    "    }\n",
    "    return data\n",
    "\n",
    "\n",
    "doc_id = 'ap~20050210-762251'\n",
    "doc = grouped_docs[doc_id]\n",
    "\n",
    "parsed = parse_doc(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 14\n",
      "['Micheál', 'Martin']\n",
      "20 20\n",
      "['Helseministeren']\n",
      "238 240\n",
      "['Denne', 'Michea´l', 'Martin']\n",
      "253 253\n",
      "['Han']\n",
      "693 694\n",
      "['Michea´l', 'Martin']\n",
      "__\n",
      "22 23\n",
      "['totalt', 'røykeforbud']\n",
      "220 220\n",
      "['Røykeforbudet']\n",
      "236 236\n",
      "['det']\n",
      "320 323\n",
      "['sitt', 'forbud', 'mot', 'røyking']\n",
      "329 331\n",
      "['den', 'nye', 'loven']\n",
      "487 489\n",
      "['den', 'nye', 'røykeloven']\n",
      "512 512\n",
      "['Dette']\n",
      "533 534\n",
      "['denne', 'loven']\n",
      "571 572\n",
      "['dette', 'forbudet']\n",
      "627 627\n",
      "['Røykeforbudet']\n",
      "639 639\n",
      "['det']\n",
      "668 668\n",
      "['Forbudet']\n",
      "713 715\n",
      "['den', 'nye', 'røykeloven']\n",
      "750 750\n",
      "['forbudet']\n",
      "798 798\n",
      "['dette']\n",
      "905 905\n",
      "['forbudet']\n",
      "__\n",
      "25 25\n",
      "['pubene']\n",
      "314 314\n",
      "['pubene']\n",
      "581 583\n",
      "['pubene', 'i', 'Irland']\n",
      "853 853\n",
      "['pubene']\n",
      "__\n",
      "35 35\n",
      "['Vi']\n",
      "72 72\n",
      "['oss']\n",
      "74 74\n",
      "['vi']\n",
      "172 172\n",
      "['vi']\n",
      "180 180\n",
      "['vi']\n",
      "190 190\n",
      "['oss']\n",
      "211 211\n",
      "['vårt']\n",
      "412 412\n",
      "['Vi']\n",
      "585 585\n",
      "['Vi']\n",
      "601 601\n",
      "['vi']\n",
      "623 623\n",
      "['oss']\n",
      "__\n",
      "40 41\n",
      "[\"Mulligan's\", 'pub']\n",
      "47 47\n",
      "[\"Mulligan's\"]\n",
      "113 113\n",
      "['puben']\n",
      "168 168\n",
      "[\"Mulligan's\"]\n",
      "176 177\n",
      "['dette', 'stedet']\n",
      "185 185\n",
      "[\"Mulligan's\"]\n",
      "__\n",
      "38 41\n",
      "['døren', 'til', \"Mulligan's\", 'pub']\n",
      "76 76\n",
      "['døren']\n",
      "__\n",
      "56 56\n",
      "['Dublin']\n",
      "416 416\n",
      "['Dublins']\n",
      "__\n",
      "54 56\n",
      "['pubene', 'i', 'Dublin']\n",
      "612 612\n",
      "['pubene']\n",
      "__\n",
      "97 97\n",
      "['bardisken']\n",
      "164 168\n",
      "['den', 'generøse', 'bardisken', 'på', \"Mulligan's\"]\n",
      "387 389\n",
      "['bardisken', 'foran', 'ham']\n",
      "__\n",
      "134 134\n",
      "['TV-skjermen']\n",
      "402 402\n",
      "['TV-skjermen']\n",
      "__\n",
      "138 139\n",
      "['Manchester', 'Uniteds']\n",
      "408 409\n",
      "['Manchester', 'United']\n",
      "__\n",
      "159 159\n",
      "['man']\n",
      "161 161\n",
      "['seg']\n",
      "__\n",
      "202 203\n",
      "['Brian', 'Gormley']\n",
      "226 226\n",
      "['Jeg']\n",
      "263 263\n",
      "['han']\n",
      "374 375\n",
      "['Brian', 'Gormley']\n",
      "389 389\n",
      "['ham']\n",
      "__\n",
      "279 279\n",
      "['Regjeringen']\n",
      "289 289\n",
      "['den']\n",
      "315 317\n",
      "['den', 'irske', 'regjeringen']\n",
      "320 320\n",
      "['sitt']\n",
      "835 835\n",
      "['regjeringen']\n",
      "839 839\n",
      "['den']\n",
      "__\n",
      "348 348\n",
      "['Irland']\n",
      "583 583\n",
      "['Irland']\n",
      "__\n",
      "428 432\n",
      "['75', 'år', 'gamle', 'Agnes', 'Duffy']\n",
      "434 434\n",
      "['sine']\n",
      "454 454\n",
      "['hun']\n",
      "461 461\n",
      "['Jeg']\n",
      "465 465\n",
      "['jeg']\n",
      "470 470\n",
      "['hun']\n",
      "481 481\n",
      "['deg']\n",
      "497 497\n",
      "['Jeg']\n",
      "508 508\n",
      "['hun']\n",
      "559 559\n",
      "['Agnes']\n",
      "566 566\n",
      "['jeg']\n",
      "__\n",
      "452 452\n",
      "['datteren']\n",
      "536 536\n",
      "['Jeg']\n",
      "556 557\n",
      "['datteren', 'Colette']\n",
      "__\n",
      "458 458\n",
      "['puben']\n",
      "521 521\n",
      "['puben']\n",
      "776 776\n",
      "['puben']\n",
      "880 880\n",
      "['puben']\n",
      "__\n",
      "574 574\n",
      "['Røykerne']\n",
      "892 892\n",
      "['Røykerne']\n",
      "__\n",
      "645 648\n",
      "['dere', 'som', 'jobber', 'her']\n",
      "654 658\n",
      "['alle', 'som', 'er', 'ansatt', 'her']\n",
      "__\n",
      "717 718\n",
      "['Linda', 'Cullen']\n",
      "725 725\n",
      "['sin']\n",
      "728 728\n",
      "['hun']\n",
      "738 738\n",
      "['jeg']\n",
      "744 744\n",
      "['jeg']\n",
      "746 746\n",
      "['Jeg']\n",
      "752 752\n",
      "['Jeg']\n",
      "865 865\n",
      "['Linda']\n",
      "882 882\n",
      "['Jeg']\n",
      "897 897\n",
      "['jeg']\n",
      "__\n",
      "781 783\n",
      "['Venninnen', 'Noreen', 'Walsh']\n",
      "790 790\n",
      "['Jeg']\n",
      "793 793\n",
      "['min']\n",
      "__\n",
      "808 808\n",
      "['bartenderen']\n",
      "812 812\n",
      "['sine']\n",
      "__\n",
      "912 912\n",
      "['Corgartys']\n",
      "939 941\n",
      "['det', 'populære', 'stedet']\n",
      "__\n",
      "927 927\n",
      "['klokken']\n",
      "929 929\n",
      "['seg']\n",
      "__\n"
     ]
    }
   ],
   "source": [
    "tokens = parsed[\"cased_words\"]\n",
    "clusters = parsed[\"clusters\"]\n",
    "\n",
    "for cluster in clusters:\n",
    "    if len(cluster) == 1:\n",
    "        continue\n",
    "    for m1, m2 in cluster:\n",
    "        print(m1, m2)\n",
    "        print(tokens[m1: m2 + 1])\n",
    "\n",
    "    print(\"__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we need to convert the data to the jsonlines format:\n",
    "def make_json(docs):\n",
    "    data = {\n",
    "        \"document_id\":      None,\n",
    "        \"cased_words\":      [],\n",
    "        \"sent_id\":          [],\n",
    "        \"part_id\":          [],\n",
    "        \"speaker\":          [],\n",
    "        \"pos\":              [],\n",
    "        \"deprel\":           [],\n",
    "        \"head\":             [],\n",
    "        \"clusters\":         []\n",
    "    }\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
